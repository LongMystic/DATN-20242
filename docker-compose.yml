services:
  # HDFS: namenode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    environment:
      - CLUSTER_NAME=datn_2025
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
      - namenode_data:/hadoop/dfs/name
    networks:
      - datn_2025_network
  
  # HDFS: datanode
  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    # environment:
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
      - datanode_data:/hadoop/dfs/data
    networks:
      - datn_2025_network
  
  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    # environment:
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
      - datanode_data:/hadoop/dfs/data
    networks:
      - datn_2025_network
  
  # YARN: 
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    hostname: resourcemanager
    deploy:
      resources:
        limits:
          memory: 4G
    environment:
      - YARN_RESOURCEMANAGER_OPTS=-Xmx3072m
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
    ports:
      - "8088:8088"
      - "8032:8032"
    networks:
      - datn_2025_network

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager1
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '2'
    environment:
      - YARN_NODEMANAGER_OPTS=-Dyarn.nodemanager.resource.memory-mb=4096 -Dyarn.nodemanager.resource.cpu-vcores=2
    privileged: true
    pid: host
    ports:
      - "8043:8042"  # NodeManager web UI
      - "45455:45454" # NodeManager localizer
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
    networks:
      - datn_2025_network

  nodemanager2:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager2
    privileged: true
    pid: host
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '2'
    environment:
      - YARN_NODEMANAGER_OPTS=-Dyarn.nodemanager.resource.memory-mb=4096 -Dyarn.nodemanager.resource.cpu-vcores=2
    ports:
      - "8044:8042"  # NodeManager web UI
      - "45456:45454" # NodeManager localizer
    volumes:
      - ./services/hadoop/conf/core-site.xml:/etc/hadoop/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      # - ./services/hadoop/conf/mapred-site.xml:/etc/hadoop/mapred-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/etc/hadoop/yarn-site.xml
    networks:
      - datn_2025_network

  # spark
  spark-history-server:
    image: apache/spark:3.3.3-python3
    container_name: spark-history-server
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer
      --conf spark.hadoop.yarn.resourcemanager.hostname=resourcemanager
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=hdfs://namenode:8020/spark-logs
      - SPARK_MASTER=yarn
      - SPARK_YARN_MODE=true
      - SPARK_YARN_JAR=hdfs://namenode:8020/spark/jars
    volumes:
      - ./services/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./services/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./services/hive/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./services/hadoop/conf/core-site.xml:/opt/spark/conf/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml
      - ./services/spark/jars/mysql-connector-java-8.0.21.jar:/opt/spark/jars/mysql-connector-java-8.0.21.jar
      - ./services/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar:/opt/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar
    ports:
      - "18080:18080"
    networks:
      - datn_2025_network
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "18080" ]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-thrift-server:
    # image: apache/spark:3.3.3-python3
    image: spark-custom:latest
    container_name: spark-thrift-server
    command: >
      /opt/spark/bin/spark-submit
      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      --master yarn
      --deploy-mode client
      --proxy-user spark_user
      --conf spark.hadoop.yarn.resourcemanager.hostname=resourcemanager
    # user: "spark_thrift_user"
    environment:
      - SPARK_MASTER=yarn
      - SPARK_YARN_MODE=true
      - SPARK_YARN_JAR=hdfs://namenode:8020/spark/jars
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - YARN_CONF_DIR=/opt/hadoop/etc/hadoop
      - HADOOP_USER_NAME=spark_user
      - SPARK_USER=spark_user
    volumes:
      - ./services/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./services/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./services/hive/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./services/hadoop/conf/core-site.xml:/opt/hadoop/etc/hadoop/conf/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/conf/hdfs-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/opt/hadoop/etc/hadoop/conf/yarn-site.xml
      - ./services/spark/jars/mysql-connector-java-8.0.21.jar:/opt/spark/jars/mysql-connector-java-8.0.21.jar
      - ./services/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar:/opt/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar
    ports:
      - "10000:10000"
      - "4040:4040"
    networks:
      - datn_2025_network
    healthcheck:
      test: [ "CMD", "nc", "-z", "localhost", "10000" ]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-client:
    image: spark-custom:latest
    container_name: spark-client
    hostname: spark-client
#    command: tail -f /dev/null  # <-- Keeps it running
    entrypoint: |
      /bin/bash -c "
      hdfs dfs -test -e /spark-jars || (
        hdfs dfs -mkdir -p /spark-jars && 
        hdfs dfs -put $SPARK_HOME/jars/* /spark-jars/
      );
      tail -f /dev/null"
    tty: true
    stdin_open: true
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_NO_DAEMONIZE=true
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - YARN_CONF_DIR=/opt/hadoop/etc/hadoop
      - HADOOP_USER_NAME=spark_user  # or any user you want
      - SPARK_LOCAL_HOSTNAME=spark-client
      - SPARK_PUBLIC_DNS=spark-client.hadoop
      - SPARK_DRIVER_HOST=spark-client
    volumes:
      - ./services/spark/conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
      - ./services/spark/conf/metrics.properties:/opt/spark/conf/metrics.properties
      - ./services/hive/conf/hive-site.xml:/opt/spark/conf/hive-site.xml
      - ./services/hadoop/conf/core-site.xml:/opt/hadoop/etc/hadoop/conf/core-site.xml
      - ./services/hadoop/conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/conf/hdfs-site.xml
      - ./services/hadoop/conf/yarn-site.xml:/opt/hadoop/etc/hadoop/conf/yarn-site.xml
      - ./services/spark/jars/mysql-connector-java-8.0.21.jar:/opt/spark/jars/mysql-connector-java-8.0.21.jar
      - ./services/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar:/opt/spark/jars/iceberg-spark-runtime-3.3_2.12-1.3.0.jar
    depends_on:
      - namenode
      - resourcemanager
    networks:
      datn_2025_network:
        aliases:
          - spark-client.hadoop
          - spark-client.yarn

  
  # hive-metastore
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    environment:
      - HIVE_CONF_DIR=/opt/hive/conf
      - HIVE_AUX_JARS_PATH=/opt/hive/lib
      - HIVE_METASTORE_PORT=9083
      - HIVE_METASTORE_HOST=hive-metastore
      - DB_TYPE=mysql
    volumes:
      - ./services/hive/conf/hive-site.xml:/opt/hive/conf/hive-site.xml
      - ./services/hive/jars/mysql-connector-java-8.0.21.jar:/opt/hive/lib/mysql-connector-java-8.0.21.jar
    entrypoint: ["/bin/bash", "-c", "
      if ! mysql -h hive-metastore-mysql -u root -proot -e 'use metastore; SELECT * FROM CTLGS LIMIT 1;' &> /dev/null; then
        echo 'Initializing schema...';
        /opt/hive/bin/schematool -dbType mysql -initSchema;
      else
        echo 'Schema already exists. Skipping initialization.';
      fi;
      /opt/hive/bin/hive --service metastore
    "]
    ports:
      - "9083:9083"
    networks:
      - datn_2025_network
    depends_on:
      mysql:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9083"]
      interval: 30s
      timeout: 10s
      retries: 3
      

  # mysql
  mysql:
    image: mysql:5.7
    container_name: mysql
    command: --default-authentication-plugin=mysql_native_password
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_DATABASE=hive_metastore
      - MYSQL_USER=hive
      - MYSQL_PASSWORD=hive
    ports:
      - "3306:3306"
    networks:
      - datn_2025_network
    healthcheck:
      test: ["CMD", "mysqladmin" ,"ping", "--password=root"]
      interval: 30s
      timeout: 10s
      retries: 3

  # superset
  superset:
    image: superset_custom:latest
    container_name: superset
    hostname: superset
    # environment:
    volumes:
      - ./services/superset/conf/superset_config.py:/app/pythonpath/superset_config.py
    ports:
      - "8088:8088"
    networks:
      - datn_2025_network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8088/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./services/prometheus/conf/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
    networks:
      - datn_2025_network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9090" ]
      interval: 30s
      timeout: 10s
      retries: 3
  
  # grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    networks:
      - datn_2025_network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3000/api/health" ]
      interval: 30s
      timeout: 10s
      retries: 3
  
networks:
  datn_2025_network:
    driver: bridge
  
volumes:
  namenode_data:
    driver: local
  datanode_data:
    driver: local